\documentclass[article]{jss}
\usepackage[utf8]{inputenc}
%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern,amsmath,amssymb}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
library("MASS")
@


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation (and optionally ORCID link)
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Toby Dylan Hocking~\orcid{0000-0002-3146-0865}\\Northern Arizona University}
\Plainauthor{Toby Dylan Hocking}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{Comparing binsegRcpp with other implementations of binary segmentation for changepoint detection}
\Plaintitle{Comparing binsegRcpp with other implementations of binary segmentation for changepoint detection}
\Shorttitle{binsegRcpp for changepoint detection}

%% - \Abstract{} almost as usual
\Abstract{ Binary segmentation is a classic algorithm for detecting
  changepoints in sequential data. In theory, using a simple loss
  function like Gaussian or Poisson, binary segmentation should be
  extremely fast for $N$ data and $K$ segments, $O(N K)$ in the worst
  case, and $O(N \log K)$ in the best case. In practice existing
  implementations such as ruptures (\proglang{Python} module) and
  changepoint (\proglang{R} package) are much slower, and in fact
  sometimes do not return a correct result. We present the
  \proglang{R} package \pkg{binsegRcpp}, which provides an efficient
  C++ implementation of binary segmentation, and always returns
  correct results. We discuss some C++ coding techniques
  which were used to avoid repetition, for efficiency, and for
  readability.  We additionally include detailed theoretical and
  empirical comparisons with other implementations of binary
  segmentation in \proglang{R} packages \pkg{fpop}, \pkg{blockcpd} and
  \pkg{wbs}. }

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{JSS, style guide, comma-separated, not capitalized, \proglang{R}}
\Plainkeywords{JSS, style guide, comma-separated, not capitalized, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Toby Dylan Hocking\\
  UniversitÃ© de Sherbrooke\\
  E-mail: \email{Toby.Hocking@R-project.org}
}

\begin{document}


\section{Introduction: previous software for change-point detection} \label{sec:intro}

TODO \code{fpop::multiBinSeg} \citep{Maidstone2017}.

TODO \code{wbs::sbs} \citep{Baranowski2019}.

TODO \code{changepoint::cpt.mean(method="BinSeg")} \citep{Killick2014,Killick2022}.

TODO \code{ruptures.Binseg} \citep{Truong2020}.

TODO \code{fastcpd} package \citep{fastcpd2024} (nice related work table).

\section{Models and software} \label{sec:models}

\subsection{C++ features}

We used several features of C++ to implement \pkg{binsegRcpp} efficiently.
In binary segmentation, a segment is split in each iteration, which results in up to two new segments that must be considered to split (see Section~\ref{sec:algo} for details).
Computing the cost of splitting a segment of size $N$ is $O(N\log N)$ time for L1/Laplace losses, and $O(N)$ time for the normal/Poisson losses.
After computing the cost for two new segments, the cost of all splittable segments must be considered, in order to identify the min cost segment to split next.
We therefore propose using two different kinds of C++ Standard Template Library containers to store and retreive the cost of segments which have previously been considered, but not yet split (Table~\ref{tab:containers}).
First, we propose using the \code{multiset} container (red-black tree), which can be used to achieve best case time complexity.
The \code{multiset} keeps segments sorted by their cost values, so offers constant $O(1)$ time retreival of the min cost segment to split next; insertion takes logarithmic $O(\log n)$ time in the number of segments $n$ currently stored in the container.
Second, we propose using the \code{list} container (doubly-linked list), which is used as a baseline, to demonstrate the benefits of the multiset container.
The \code{list} keeps segments stored in the order in which they were inserted, so offers constant $O(1)$ time insertion of new elements, and linear $O(n)$ time retreival of the min cost segment to split, for a container with $n$ segments currently stored.

A C++ virtual class \code{Container} is used to represent the methods that must be provided by either type of container: \code{insert}, \code{get_best}.
We used a C macro, \code{CMAKER}, to declare the two sub-classes:
\begin{verbatim}
#define CMAKER(CONTAINER, INSERT, GET_BEST)...
CMAKER(multiset, insert, segment_container.begin())
CMAKER(list, push_back, std::min_element(
 segment_container.begin(),segment_container.end()))
\end{verbatim}
The code above is expanded by the C pre-processor, to two declarations of \code{Container} sub-classes.

Another C macro is used to declare the loss functions which require the cumulative median:
\begin{verbatim}
#define ABS_DIST(NAME, VARIANCE)...
ABS_DIST(l1, false)
ABS_DIST(laplace, true)
\end{verbatim}
The loss function which requires a variance estimate is the Laplace negative log likelihood; the simple L1 loss is obtained without a variance estimate.
A \code{class absDistribution} implements the common operations for the two loss functions, including the cumulative median in $O(N\log N)$ time using the algorithm of \citet{Drouin2017mmit}.

Another C macro is used to declare the loss functions which require the cumulative sum:
\begin{verbatim}
#define CUM_DIST(NAME, COMPUTE, ERROR, VARIANCE)...
CUM_DIST(mean_norm, RSS, , false)
CUM_DIST(poisson,
 (mean>0) ? (mean*N - log(mean)*sum) : ( (sum==0) ? 0 : INFINITY ),
 if(round(value)!=value)return ERROR_DATA_MUST_BE_INTEGER_FOR_POISSON_LOSS;
 if(value < 0)return ERROR_DATA_MUST_BE_NON_NEGATIVE_FOR_POISSON_LOSS;,
 false)
CUM_DIST(meanvar_norm,
 (var>max_zero_var) ? (RSS/var+N*log(2*M_PI*var))/2 : INFINITY,
 , true)
\end{verbatim}
The code below declares three loss functions based on the cumulative sum: \code{mean_norm} is the residual sum of squares (RSS), \code{poisson} loss is for count data, and \code{meanvar_norm} is the normal model with change in mean and variance.
The \code{COMPUTE} parameter specifies the loss, in terms of six variables: \code{mean}, \code{var}, \code{N}, \code{sum}, \code{squares}, \code{max_zero_var} (Table~\ref{tab:cum-params}).
These variables are computed in a \code{class CumDistribution} which contains the logic common to these loss functions (cumulative sum, etc).
Note that \code{mean} and \code{var} parameters  code can compute the loss of held-out validation data (not used to compute model parameters), with respect to a subset TODO. The \code{ERROR} parameter is an optional block of code, that is used to return an error status code, if there are unusual input data.

We assume there is a sequence of $N$ data $x_1,\dots,x_N\in\mathbb R$,
with corresponding weights $w_1,\dots,w_N>0$. Binary segmentation can
be interpreted as attempting to

To define the loss functions based on the cumulative sum

\begin{table}
  \begin{tabular}{ccc}
    Parameter & Description & Definition \\
    \hline
    $M=$\code{mean} & model parameter & $T/W$ \\
    \code{var} & model parameter &  $S/W + M(M-2T/W)$
    
  \end{tabular}
\end{table}

Additionally, the \code{CUM_DIST} and \code{ABS_DIST} macros generate code that populates a static \code{unordered_map} which can be queried to obtain a list of distributions which are supported by the C++ code:
%
<<>>=
binsegRcpp::get_distribution_info()$dist
@ 
%
To illustrate the performence benefits of using log-time  containers  were used to efficiently , virtual classes, static
variables, function pointers, templates, macros).




\begin{table}[t!]
\centering
\begin{tabular}{l|ccccc}
  package & binsegRcpp & changepoint & wbs & fpop & ruptures \\
  function & binseg & cpt.mean & sbs & multiBinSeg & Binseg \\
%   > ruptures$version$version
% [1] "1.1.6"
% > sapply(c("binsegRcpp", "changepoint", "wbs", "fpop"), function(p)paste(packageVersion(p)))
%  binsegRcpp changepoint         wbs        fpop 
% "2022.3.29"     "2.2.3"       "1.4" "2019.8.26" 
  version & 2022.3.29 & 2.2.3 & 1.4 &2019.8.26 & 1.1.6\\
\hline
  weights & yes & no & no & no & no \\
  max segs & yes & yes & no & yes & yes \\
  dim & one & one & one & multi & multi \\
  correct & yes & no & yes & yes & no \\
  losses & C++ & C & L2 & L2 & Python \\
  % all propose some loss computation.
  storage & STL multiset & arrays & recursion & heap & LRU cache \\
  space & $O(S)$ & $O(S^2)$ & $O(S)$ & $O(S)$ & $O(S)$ \\
  cumsum & yes & yes & yes & yes & no \\
  best & $O(N\log N)$ & $O(N^3)$ & $O(N\log N)$ & $O(N\log N)$ & $>O(N\log N)$ \\
  worst & $O(N^2)$ & $O(N^3)$ & $O(N^2)$ & $O(N^2)$ & $O(N^2)$ \\
  CV & yes & no & no & no & no\\
  params & yes & largest & no & no & no \\
\end{tabular}
\caption{\label{tab:overview} TODO}
\end{table}

\paragraph{Loss computation.} We run each algorithm on \code{N.data}
points up to \code{max.segments =
  max.changes+1}. \code{binsegRcpp::binseg} result is a list which
contains a data table with \code{max.segments} rows and column
\code{loss} that is the square loss.  \code{changepoint::cpt.mean}
result is a list of class \code{cpt.range} with method \code{logLik}
which returns the square loss of one of the models. \code{wbs::sbs}
result is a list which contains a data frame with \code{N.data-1} rows
and \code{CUSUM} and \code{min.th} columns
TODO.
\code{fpop::multiBinSeg} result is a list with element
\code{J.est}, which is a vector of \code{max.changes} square loss
decrease values.  \code{ruptures.Binseg.predict} result is a vector of
segment ends for one model size, which can be passed to
\code{sum_of_costs} method to compute the square loss.

Three packages have implemented the normal change in mean and variance
model. \code{binsegRcpp::binseg} loss values are the normal negative
log likelihood (NLL). The \code{changepoint::logLik} function returns two
times the NLL. The \code{ruptures.Binseg}
loss is related via this equation,

\begin{equation}
  \text{NLL} = (\text{rupturesLoss} + N[1+\log(2\pi)])/2
\end{equation}

\begin{table}
  \centering
  \begin{tabular}{c|cc|cc|cc}
    case/splits: & \multicolumn{2}{c}{one } & \multicolumn{2}{c}{best/equal} & \multicolumn{2}{c}{worst/unequal } \\
    operation: & insert &
                             %remove &
                                      argmin & insert & argmin & insert & argmin\\
    \hline
    list & $O(1)$ &
    % $O(1)$ &
               $O(n)$ &
    $O(S\log S)$ & $O(S)$ & $O(S)$ & $O(S)$\\
    heap/multiset & $O(\log n)$ &
    % $O(\log N)$ &
                    $O(1)$ &
                             $O(S)$ & $O(S^2)$ & $O(S)$ & $O(S)$\\
    \hline
    Find new split & \multicolumn{2}{c}{} & \multicolumn{2}{c}{$O(N\log S)$} & \multicolumn{2}{c}{$O(NS)$}
  \end{tabular}
  \caption{container properties}
  \label{tab:containers}
\end{table}


\section{Illustrations} \label{sec:illustrations}

TODO
%
<<data>>=
data("quine", package = "MASS")
@
%
TODO

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-neuroblastoma.png}
  \caption{\label{fig:neuroblastoma} For a real cancer DNA copy number
    data set with 273 observations, we show the first four
    changepoints detected by several different implementations of
    binary segmentation.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings.png}
\caption{\label{fig:timings} Timings using square loss.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings-laplace.png}
\caption{\label{fig:timings-laplace} Timings using L1 loss.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings-meanvar_norm.png}
\caption{\label{fig:timings-meanvar_norm} Timings using Gaussian change in mean and variance model.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings-poisson.png}
\caption{\label{fig:timings-poisson} Timings using Poisson loss.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-compare-distributions.png}
  \caption{\label{fig:compare-distributions} Timings using binsegRcpp multiset with
    several different loss functions.}
\end{figure}

\section{Summary and discussion} \label{sec:summary}

\begin{leftbar}
As usual \dots
\end{leftbar}

\bibliography{refs}

\newpage

\begin{appendix}

  \section{More technical details} \label{app:technical}
  TODO

\end{appendix}


\end{document}
