\documentclass[article]{jss}
\usepackage[utf8]{inputenc}
%% -- LaTeX packages and custom commands ---------------------------------------
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\, min}
%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern,amsmath,amssymb}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 80, useFancyQuotes = FALSE)
library("MASS")
@


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation (and optionally ORCID link)
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Toby Dylan Hocking~\orcid{0000-0002-3146-0865}\\Université de Sherbrooke}
\Plainauthor{Toby Dylan Hocking}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{Comparing \pkg{binsegRcpp} with other implementations of binary segmentation for change-point detection}
\Plaintitle{Comparing binsegRcpp with other implementations of binary segmentation for change-point detection}
\Shorttitle{binsegRcpp for efficient change-point detection}

%% - \Abstract{} almost as usual
\Abstract{
  Binary segmentation is a classic algorithm for detecting
  change-points in sequential data. In theory, using a simple loss
  function like Normal or Poisson, binary segmentation should be
  extremely fast for $N$ data and $K$ segments: asymptotically $O(N K)$ time in the worst
  case, and $O(N \log K)$ time in the best case.
  In practice,
  other implementations can be asymptotically slower, and can 
  return incorrect results. We propose \pkg{binsegRcpp}, an \proglang{R} package which
  provides a correct \proglang{C++} implementation, with the expected asymptotic time complexity.
  %% In practice, existing
  %% implementations such as \pkg{ruptures} (\proglang{Python} module) and
  %% \pkg{changepoint} (\proglang{R} package) are asymptotically slower.
  %% Additionally, \pkg{changepoint} sometimes returns incorrect results.
  %% We present the \proglang{R} package \pkg{binsegRcpp}, which provides an efficient
  %% and correct C++ implementation of binary segmentation.
  We discuss several important \proglang{C++} coding techniques,
  and include detailed comparisons with other implementations of binary
  segmentation: \pkg{ruptures}, \pkg{fpop}, \pkg{changepoint}, \pkg{blockcpd}, and
  \pkg{wbs}.
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{\proglang{C++}, \proglang{R}, binary segmentation, change-point}
\Plainkeywords{C++, R, binary segmentation, change-point}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Toby Dylan Hocking\\
  Université de Sherbrooke\\
  E-mail: \email{Toby.Hocking@R-project.org}
}

\begin{document}


\section{Introduction: previous software for change-point detection} \label{sec:intro}

TODO \code{fpop::multiBinSeg} \citep{Maidstone2017}.

TODO \code{wbs::sbs} \citep{Baranowski2019}.

TODO \code{changepoint::cpt.mean(method="BinSeg")} \citep{Killick2014,Killick2022}.

TODO \code{ruptures.Binseg} \citep{Truong2020}.

TODO \code{fastcpd} package \citep{fastcpd2024} (nice related work table).

TODO \pkg{blockcpd} package TODO \citep{blockcpd2021,Prates2021} .

\section{Models and software} \label{sec:models}

We assume there is a sequence of $N$ data $x_1,\dots,x_N\in\mathbb R$, in which we want to find abrupt changes.
We assume that each data point $i\in\{1,\dots,N\}$ has a corresponding weight $w_i>0$.
Binary
segmentation can be interpreted as attempting to find model parameters
$\theta$ which minimize a given weighted loss function $w_i
\ell(\theta, x_i)$. For example, minimizing the square loss $w_i
\ell(\theta, x_i) = w_i (\theta-x_i)^2$ corresponds to maximizing the
normal log-likelihood, when the variance is uniform.

\paragraph{Choice of observation-specific weights.}
Weights can be used to efficiently handle two particular kinds of data.
First, in the case of data which are sampled uniformly across
space/time, typically the weights are constant ($w_i=1$ for all $i$),
but other weights can be used for data which are sampled non-uniformly
(for example daily time series with some missing days).
Second, in the case of data with runs of identical values, a run-length encoding can be used to convert the raw/unweighted data sequence into a compressed/weighted data sequence that can be more efficiently processed.
For example, in genomics, DNA sequence reads are aligned to a reference genome, and a coverage profile is computed by counting the number of aligned reads at each reference position.
Since each DNA sequence read is about 100 bases long, the coverage profile has runs of identical values, which can be converted to a run-length encoding for $\approx 100\times$ speedups.

\paragraph{Min loss.}
The iterative algorithm involves computing the min loss over
candidate segments, each of which is defined by a start $j$ and end
$e$ position ($1<j<e<N$).

\begin{equation}
L_{j,e} = \min_{\theta} \sum_{i=j}^e w_i \ell(\theta, x_i).
\end{equation}
The minimization problem is over the model parameter $\theta$ (segment-specific center and possibly scale).

\paragraph{Best loss difference.}
Since different segments have different loss values before and after splitting, the algorithm must choose the next segment to split by minimizing a loss difference value.
Let $D_{j,e}(t)=L_{j,t}+L_{t+1,n}-L_{j,e}<0$ be the loss difference after splitting segment $(j,e)$ after $t$.
The $L_{j,e}$ term is the loss before the split, and $L_{j,t}+L_{t+1,n}$ is the loss after the split, which is necessarily smaller.
Let $\delta_{j,e},\tau_{j,e}=\min,\argmin_{t\in \{j,\dots,e-1\}} D_{j,e}(t)$ be the best loss difference/split on segment $(j,e)$.
%% We can re-write this minimization as
%% \begin{equation}
%%   f_{j,e}=\min,\argmin -L_{j,e}+ \begin{cases}
%%     D_{j,e}(j) = L_{j,j}+L_{j+1,e} \\
%%     %D_{j,e}(j+1) = L_{j,j+1}+L_{j+2,e} \\
%%     \vdots\\
%%     D_{j,e}(e-1) = L_{j,e-1}+L_{e,e} \\
%%   \end{cases}
%%   \label{eq:fje}
%% \end{equation}
\paragraph{Efficient computation.}
To compute the loss differences $D_{j,e}(j),\dots,D_{j,e}(e-1)$, we need to compute the sequence of values $L_{j,j}, \dots, L_{j,e-1}$ (loss values for possible new segments before candidate split points), and the sequence of values $L_{j+1,e}, \dots, L_{e,e}$ (loss values for possible new segments after candidate split points).
These sequences can be computed efficiently for the five loss functions that we implemented in \code{binsegRcpp}.
The time complexity of computing these sequences depends on the loss function, and the size of the segment to split, $s=e-j+1$.
For the case of negative log likelihood of Normal and Poisson distributions, the cumulative sum trick can be used to compute these sequences in linear $O(s)$ time.
For the case of the L1 loss and negative log likelihood of Laplace distribution, these sequences can be computed efficiently in log-linear $O(s\log s)$ time using the cumulative median algorithm of \citet{Drouin2017mmit}.

\subsection{Algorithm pseudo-code}

The binary segmentation algorithm needs to keep track of a set of segments that can be split, $\mathcal S_k$ for iteration $k$.
Each segment $(j,e,d,t)\in\mathcal S_k$ is represented by the start/end positions $(j,e)$ along with the corresponding min loss difference $d<0$, as well as the best split point $t\in\{j,\dots,e-1\}$.

\paragraph{Initialization.}
The initialization of the algorithm is as follows.
Let $\mathcal L_1=L_{1,n}$ be the loss with one segment,
and let $\delta_1,t_1= f_{1,n}$ be the best loss difference and split point for the entire data.
We initialize the set of segments to split as $\mathcal S_1=\{(1,N,d_1,t_1)\}$.

\paragraph{Recursive update rules.}
Let $K\in\{ 2, \dots, N\}$ be the user-specified max number of segments to consider.
The binary segmentation algorithm recursively computes the following quantities for each $k\in\{2,\dots,K\}$.
First, the best segment to split is computed by greedy minimization of the loss decrease over all splittable segments.
\begin{equation}
  j^*_k, e^*_k, d^*_k, t^*_k = \argmin_{(j,e,d,t)\in\mathcal S_{k-1}} d.
\end{equation}
Ideally, the minimization above takes $O(1)$ constant time, if the set of splittable segments is a red-black tree (C++ STL multiset).
As a baseline, we also consider a linked list, which results in a sub-optimal $O(k)$ linear time search (see Section~\ref{sec:data-structures} for details).

Second, the total loss is computed by adding the best loss decrease $d^*_k$ to the best loss $L_{k-1}$ at the previous iteration, 
\begin{equation}
  \mathcal L_k=\mathcal L_{k-1} +d^*_k.
\end{equation}

Third, we construct a set of two new segments to consider:
\begin{equation}
  \mathcal N_k=\{
  (j^*_k, t^*_k),
  (t^*_k+1,e^*_k)
  \}.
\end{equation}

Fourth, we subset the two new segments, to consider only the segments that could possibly be split:
\begin{equation}
  \label{eq:Vk}
  \mathcal V_k=\{(j,e,f_{j,e})\mid (j,e)\in\mathcal N_k,\, j<e\}.
\end{equation}

Finally, we update the set of splittable segments, by removing the currently split segment, and inserting the new segments that could be split:
\begin{equation}
  \mathcal S_k=
[\mathcal S_{k-1}\setminus (j^*_k, e^*_k, d^*_k, t^*_k)]
\cup\mathcal V_k.
\end{equation}
TODO time.

\subsection{Modification for min segment length parameter}

Sometimes there is prior knowledge that there should be no segments
with fewer data points than $m\in\{1,2,\dots\}$. In that case, $m$ is
referred to as a min segment length parameter, and there are two simple
modifications to the equations above.
First, the best split (\ref{eq:fje}) is re-defined as
\begin{equation}
f^m_{j,e}=\min,\argmin_{t\in\{
j+m-1,\dots,e-m
\}} D_{j,e}(t).
\end{equation}
Second, the set of splittable segments (\ref{eq:Vk}) is re-defined as
\begin{equation}
  \mathcal V^m_k=\{(j,e,f_{j,e})\mid (j,e)\in\mathcal N_k,\, 
  e-j+1\geq 2m
  \}.
\end{equation}



\subsection{Choice of container}

We used several features of C++ to implement \pkg{binsegRcpp} efficiently.
In binary segmentation, a segment is split in each iteration, which results in up to two new segments that must be considered to split (see Section~\ref{sec:algo} for details).
Computing the cost of splitting a segment of size $N$ is $O(N\log N)$ time for L1/Laplace losses, and $O(N)$ time for the normal/Poisson losses.
After computing the cost for two new segments, the cost of all splittable segments must be considered, in order to identify the min cost segment to split next.
We propose using three different kinds of C++ Standard Template Library containers to store and retreive the cost of segments which have previously been considered, but not yet split (Table~\ref{tab:containers}).
First, we propose using the \code{multiset} container (red-black tree) and \code{priority\_queue} container (heap), which can be used to achieve best case time complexity.
These containers keep segments sorted by their cost values, so offer constant $O(1)$ time retreival of the min cost segment to split next; insertion takes logarithmic $O(\log n)$ time in the number of segments $n$ currently stored in the container.
Second, we propose using the \code{list} container (doubly-linked list), which is used as a baseline, to demonstrate the benefits of the other containers.
The \code{list} keeps segments stored in the order in which they were inserted, so offers constant $O(1)$ time insertion of new elements, and linear $O(n)$ time retreival of the min cost segment to split, for a container with $n$ segments currently stored.

%% A C++ virtual class \code{Container} is used to represent the methods that must be provided by either type of container: \code{insert}, \code{get_best}.
%% We used a C macro, \code{CMAKER}, to declare the two sub-classes:
%% \begin{verbatim}
%% #define CMAKER(CONTAINER, INSERT, GET_BEST)...
%% CMAKER(multiset, insert, segment_container.begin())
%% CMAKER(list, push_back, std::min_element(
%%  segment_container.begin(),segment_container.end()))
%% \end{verbatim}
%% The code above is expanded by the C pre-processor, to two declarations of \code{Container} sub-classes.

\subsection{Choice of loss function}

C macros are used to declare the different loss functions.
For loss functions which require the cumulative median, we use the C++ code below
\begin{verbatim}
#define ABS_DIST(NAME, VARIANCE)...
ABS_DIST(laplace, true)
ABS_DIST(l1, false)
\end{verbatim}
The loss function which requires a variance estimate is the Laplace negative log likelihood; the simple L1 loss is obtained without a variance estimate.
A \code{class absDistribution} implements the common operations for the two loss functions, including the cumulative median in $O(N\log N)$ time using the algorithm of \citet{Drouin2017mmit}.

Another C macro is used to declare the loss functions which require the cumulative sum:
\begin{verbatim}
#define CUM_DIST(NAME, COMPUTE, ERROR, VARIANCE)...
CUM_DIST(mean_norm, RSS, , false)
CUM_DIST(meanvar_norm,
 (var>max_zero_var) ? (RSS/var+N*log(2*M_PI*var))/2 : INFINITY,
 , true)
CUM_DIST(poisson,
 (mean>0) ? (mean*N - log(mean)*sum) : ( (sum==0) ? 0 : INFINITY ),
 if(round(value)!=value)return ERROR_DATA_MUST_BE_INTEGER_FOR_POISSON_LOSS;
 if(value < 0)return ERROR_DATA_MUST_BE_NON_NEGATIVE_FOR_POISSON_LOSS;,
 false)
\end{verbatim}
The code above declares three loss functions based on the cumulative sum: \code{mean_norm} is the residual sum of squares (RSS), \code{poisson} loss is for count data, and \code{meanvar_norm} is the normal model with change in mean and variance.
The \code{COMPUTE} parameter specifies the loss, in terms of six variables: \code{mean}, \code{var}, \code{N}, \code{sum}, \code{squares}, \code{max_zero_var} (Table~\ref{tab:cum-params}).
These variables are computed in a \code{class CumDistribution} which contains the logic common to these loss functions (cumulative sum, etc).
Note that the code supports cross-validation, in the sense that the input train set can be divided into a subtrain set (used to compute model parameters) and a validation set (used to evaluate the computed model parameters). The \code{mean} and \code{var} variables are model parameters, computed using the subtrain set, whereas the other variables depend on the data (subtrain or validation set).
The \code{ERROR} parameter is an optional block of code, that is used to return an error status code, if there are unusual input data.

To define the loss functions based on the cumulative sum TODO
\begin{table}
  \begin{tabular}{ccc}
    Parameter & Description & Definition \\
    \hline
    $M=$\code{mean} & model parameter & $T/W$ \\
    \code{var} & model parameter &  $S/W + M(M-2T/W)$
  \end{tabular}
  \caption{\label{tab:params}TODO}
\end{table}

\begin{table*}
  \centering
  \begin{tabular}{ccccccc}
    Name & Loss & Distrib. & Center & Scale & Params & Complexity \\
    \hline
    \texttt{mean\_norm} & Square & Normal & yes & no & 1 & $O(S)$ \\
    \texttt{meanvar\_norm} & Neg. Log Lik. & Normal & yes & yes & 2 & $O(S)$ \\
    \texttt{poisson} & Neg. Log Lik. & Poisson & yes & yes & 1 & $O(S)$ \\
    \texttt{l1} & Absolute & Laplace & yes & no & 1 & $O(S\log S)$ \\
    \texttt{laplace} & Neg. Log Lik & Laplace & yes & yes & 2 & $O(S\log S)$ \\
  \end{tabular}
  \caption{\label{tab:losses}
    TODO
  }
\end{table*}

Additionally, the \code{CUM_DIST} and \code{ABS_DIST} macros generate code that populates a static \code{unordered_map} which can be queried to obtain a list of distributions which are supported by the C++ code:
%
<<>>=
binsegRcpp::get_distribution_info()$dist
@ 
%
To illustrate the performence benefits of using log-time  containers  were used to efficiently , virtual classes, static
variables, function pointers, templates, macros).




\begin{table}[t!]
  \centering
  \small
\begin{tabular}{l|cccccc}
  package & binsegRcpp & changepoint & wbs & fpop & ruptures & blockcpd\\
  function & binseg & cpt.mean & sbs & multiBinSeg & Binseg & fit\_blockcpd\\
%   > ruptures$version$version
% [1] "1.1.6"
% > sapply(c("binsegRcpp", "changepoint", "wbs", "fpop"), function(p)paste(packageVersion(p)))
%  binsegRcpp changepoint         wbs        fpop 
% "2022.3.29"     "2.2.3"       "1.4" "2019.8.26"
% https://github.com/HaotianXu/changepoints does not have binary segmentation.
  version & 2022.3.29 & 2.2.3 & 1.4 & 2019.8.26 & 1.1.9 & 1.0.0\\
\hline
  weights & yes & no & no & no & no & no\\
  max segs & yes & yes & no & yes & yes & yes\\
  dim & one & one & one & multi & multi & multi \\
  correct & yes & no & yes & yes & yes & yes\\
  losses & 5 & 6 & 1 & 1 & 10 & 5\\
  language & C++ & C & C & C++ & Python & C++\\
  % all propose some loss computation.
  storage & heap & arrays & recursion & heap & LRU cache & heap \\
  space & $O(S)$ & $O(S^2)$ & $O(S)$ & $O(S)$ & $O(S)$ & $O(S)$ \\
  cumsum & yes & yes & yes & yes & no & yes\\
  best & $O(N\log N)$ & $O(N^3)$ & $O(N\log N)$ & $O(N\log N)$ & $O(N^{1.4})$ & $O(N\log N)$\\
  worst & $O(N^2)$ & $O(N^3)$ & $O(N^2)$ & $O(N^2)$ & $O(N^2)$ & $O(N^2)$\\
  CV & yes & no & no & no & no & no\\
  params & all & one & no & no & no &one\\
\end{tabular}
\caption{\label{tab:overview}
  Properties (rows) of six binary segmentation libraries (columns).
Properties are weights (can specify observation-specific positive weights for the loss), max segs (can specify maximum number of segments parameter), dim (dimensions of input data), correct (does code return correct model parameters?), losses (number of loss functions implemented), language (programming language to implement loss), storage (technique used to store cost of splitting a segment), space (space complexity in number of segments $S$), cumsum (cumulative sum used), best (time complexity in best case data), worst (time complexity in worst case data), CV (support for cross-validation), params (parameters are returned). }
\end{table}

\paragraph{Loss computation.} We run each algorithm on \code{N.data}
points up to \code{max.segments =
  max.changes+1}. \code{binsegRcpp::binseg} result is a list which
contains a data table with \code{max.segments} rows and column
\code{loss} that is the square loss.  \code{changepoint::cpt.mean}
result is a list of class \code{cpt.range} with method \code{logLik}
which returns the square loss of one of the models. \code{wbs::sbs}
result is a list which contains a data frame with \code{N.data-1} rows
and \code{CUSUM} and \code{min.th} columns
TODO.
\code{fpop::multiBinSeg} result is a list with element
\code{J.est}, which is a vector of \code{max.changes} square loss
decrease values.  \code{ruptures.Binseg.predict} result is a vector of
segment ends for one model size, which can be passed to
\code{sum_of_costs} method to compute the square loss.

Three packages have implemented the normal change in mean and variance
model. \code{binsegRcpp::binseg} loss values are the normal negative
log likelihood (NLL). The \code{changepoint::logLik} function returns two
times the NLL. The \code{ruptures.Binseg}
loss is related via this equation,

\begin{equation}
  \text{NLL} = (\text{rupturesLoss} + N[1+\log(2\pi)])/2
\end{equation}

\begin{table}
  \centering
  \begin{tabular}{c|cc|cc|cc}
    case/splits: & \multicolumn{2}{c}{one } & \multicolumn{2}{c}{best/equal} & \multicolumn{2}{c}{worst/unequal } \\
    operation: & insert &
                             %remove &
                                      argmin & insert & argmin & insert & argmin\\
    \hline
    list & $O(1)$ &
    % $O(1)$ &
               $O(n)$ &
    $O(S\log S)$ & $O(S)$ & $O(S)$ & $O(S)$\\
    heap/multiset & $O(\log n)$ &
    % $O(\log N)$ &
                    $O(1)$ &
                             $O(S)$ & $O(S^2)$ & $O(S)$ & $O(S)$\\
    \hline
    Find new split & \multicolumn{2}{c}{} & \multicolumn{2}{c}{$O(N\log S)$} & \multicolumn{2}{c}{$O(NS)$}
  \end{tabular}
  \caption{container properties}
  \label{tab:containers}
\end{table}

\section{Storage and retreival of segment-specific model parameters}

To simplify the discussion in this section, we use ``segment mean'' to represent arbitrary segment-specific model parameters.
For example, segment-specific model parameters can be the mean/variance of the normal distribution, or median/scale of the Laplace distribution.
A unique feature that is provided by \pkg{binsegRcpp}, but not provided by other implementations of binary segmentation, is efficient storage and retrieval of arbitrary segment mean parameters.
In contrast, \code{changepoint::cpt.mean} returns only the segment means for the selected model, and other implementations do not return any segment means (but the returned change-point variables could be used to compute estimates).

%data_vec <- c(5, 0, 20, 15, -10, 0)
For example, we consider a simple example with six data points.
<<>>=
ex6_df <- data.frame(value = c(1, -7, 8, 10, 2, 4))
ex6_binsegRcpp <- binsegRcpp::binseg(
  "mean_norm", ex6_df$value, max.segments = 4)
ex6_binsegRcpp$splits[, .(segments, end, before.mean, after.mean,
  invalidates.index, invalidates.after)]
@
The output above is a table with four rows, one for each model size.
For a max number of segments $M$, this representation of model parameters uses $M$ rows, so is asymptotically linear, $O(M)$.
The first row represents the trivial model: one segment across all data points, with \code{end=6} equal to the index of the last data point.
The model parameter of the trivial model is \code{before.mean=3}, which is the mean of all of the data.
The other rows represent the segments that result from the recursive splitting algorithm.
For example, in the second row, \code{end=2} means that the first change-point results in a split between data points 2 and 3.
The \code{before.mean = -3} is the mean up to data point 2, and \code{after.mean=6} is the mean from data point 3 to the end.
The other columns, \code{invalidates.index} and \code{invalidates.after}, indicate the previous means which are no longer valid, for models with the given split.
For example, in the second row, \code{invalidates.index=1} identifies the first row, and \code{invalidates.after=0} identifies the mean before the split (not after), which implies that \code{before.mean=3} is no longer valid, after performing the split in the second row.
In the third row, we see a new split at \code{end=4}, resulting in new parameters \code{before.mean=9} and \code{after.mean=3}, which invalidate the previous mean in row 2 after the split (\code{after.mean=6}), while the other mean in row 2 remains valid (\code{before.mean = -3}).

In general, for any number of segments $K$, we can compute a table of
estimated means (one row for each segment), using the following
algorithm.  First, consider only the first $K$ rows of the
\code{splits} table.  Next, set values in \code{before.mean} and
\code{after.mean} columns to missing (\code{NA}), based on values in
\code{invalidates.index} and \code{invalidates.after} columns.
Finally, sort the \code{end} values, and use the corresponding
non-missing mean values.
For $K$ segments, this algorithm is $O(K \log K)$ time due to the sort.
It is implemented in the \code{coef} method, as shown in the example code below:

<<>>=
(ex6_segments <- coef(ex6_binsegRcpp, 2:4))
@

The table above contains one row per segment, for three model sizes (two to four segments).
It can be used to plot the segment means and change-point variables, using the code below.

<< ex6, fig=TRUE, height=3 >>=
library(ggplot2)
ex6_df$position <- 1:nrow(ex6_df)
ggplot() + geom_point(aes(position, value), data = ex6_df) +
  geom_segment(aes(start.pos, y = mean, xend = end.pos, yend = mean),
    color = "green", data = ex6_segments) +
  geom_vline(aes(xintercept = start.pos), color="green",
    linetype = "dashed", data = ex6_segments[1 < start]) +
  facet_grid(segments ~ ., labeller = label_both)
@ 

The figure above shows the data points in black, along with the segmentation model in green (dashed vertical change-points, and solid horizontal segment means).

\subsection{Comparison with changepoint}

To do the analogous computation with \pkg{changepoint}, we can use the code below.

<<>>=
ex6_changepoint <- changepoint::cpt.mean(ex6_df$value,
  method = "BinSeg", Q = 3, penalty = "Manual", pen.value = 0)
changepoint::cpts.full(ex6_changepoint)
@

The output above is the matrix of change-points, one row for each iteration of binary segmentation.
One issue with the output above is that the change-points are incorrect in rows 2 and 3.
For example, row 2 represents the model with two change-points, which appear as 0 and 2 in the matrix above.
However, for the model with two change-points, the expected result is segments that end at data points 2, 4, and 6.
These results indicate that the implementation of binary segmentation in \pkg{changepoint} can yield incorrect change-point detection results.

Another issue with \pkg{changepoint} is that the storage of change-points is relatively inefficient.
For a maximum number of change-points $Q$, it uses a $Q\times Q$ matrix of integers to represent the change-points, so the storage space is quadratic $O(Q^2)$.
In contrast, \pkg{binsegRcpp} uses only linear space, $O(Q)$, which is asymptotically more efficient.
Therefore, \pkg{changepoint} can only handle models with relatively small numbers of change-points, whereas \pkg{binsegRcpp} can efficiently store arbitrarily large change-point models.

A final issue is illustrated via the code below,
%rbind(changepoint = changepoint::param.est(ex6_changepoint)$mean,
%      expected = ex6_segments[segments==3, mean])
<<>>=
changepoint::param.est(ex6_changepoint)
@

The result above shows the estimated segment means, for which there are two issues.
First, there are only three values, so these should be the segment means for the model with two change-points.
However, these three values are incorrect (the correct segment means are -3, 9, and 3).
Another issue is that the segment means are only reported for one model size,
and it is not possible to retreive means for other model sizes.

\subsection{Comparison with wbs}

Like \pkg{binsegRcpp}, the \pkg{wbs} package provides a correct implementation of binary segmentation.
However, it is missing some key features, such as the ability to specify a max number of segments.
The \pkg{wbs} code below computes the full path of binary segmentation models.

<<>>=
ex6_wbs <- wbs::sbs(ex6_df$value)
data.table::data.table(ex6_wbs$res)[order(-min.th)]
@

The \code{res} table above is similar to the \code{splits} table from \pkg{binsegRcpp}.
It has one row per split in binary segmentation, so there are five rows in this example (because there are six data points).
The \code{cpt} column is the index of the last data point before the detected change-point, which is consistent with the \code{end} column from \pkg{binsegRcpp}.
The \pkg{wbs} package implements binary segmentation using recursive function calls in C (depth-first search, which means it is impossible to specify a max number of segments).
To compute the change-points for some number of change-points $Q$, the \code{res} table must be sorted, and only the top $Q$ rows be kept.
For example, the model with $Q=2$ change-points can be represented using the first two rows of the table above (change-points at 2 and 4).
However, the \pkg{wbs} package does not support retreival of segment-specific model parameters such as segment means.
If the user wants segment mean estimates, they must be computed using the returned change-points.

\section{Illustrations} \label{sec:illustrations}

TODO
%
<<data>>=
data("quine", package = "MASS")
@
%
TODO
 
\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{changepoint.bug.png}
  \caption{\label{fig:neuroblastoma} For a real cancer DNA copy number
    data set with 273 observations, we show the first four
    change-points models for two different R packages that provide code for
    binary segmentation: \pkg{binsegRcpp} and \pkg{changepoint}.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings.png}
\caption{\label{fig:timings} Timings using square loss.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings-laplace.png}
\caption{\label{fig:timings-laplace} Timings using L1 loss.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings-meanvar_norm.png}
\caption{\label{fig:timings-meanvar_norm} Timings using Gaussian change in mean and variance model.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings-poisson.png}
\caption{\label{fig:timings-poisson} Timings using Poisson loss.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-compare-distributions.png}
  \caption{\label{fig:compare-distributions} Timings using binsegRcpp multiset with
    several different loss functions.}
\end{figure}

\section{Summary and discussion} \label{sec:summary}

\begin{leftbar}
As usual \dots
\end{leftbar}

\bibliography{refs}

\newpage

\begin{appendix}

  \section{More technical details} \label{app:technical}

  
\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-heap-pred}
  \caption{\label{fig:heap-pred} Comparing best-case computation time of \pkg{binsegRcpp} with three containers, and two baselines: \pkg{changepoint} and \pkg{ruptures}.
    Square loss and best case data were used (integers from 1 to $N$, result in equal splits).
  }
\end{figure}

\begin{figure}[t!]
  \centering
  %\includegraphics[width=\textwidth]{figure-heap-refs-binsegRcpp}
  \input{figure-heap-refs-binsegRcpp}
  \caption{\label{fig:heap-refs}
    Estimating best-case asymptotic complexity for time (seconds) and memory (kilobytes).
    Black curves are empirical measurements for \pkg{binsegRcpp} with two containers, and violet curves are references.
    These data suggest that both list and multiset containers are $O(N)$ space
    list is $O(N^2)$ time and multiset is $O(N\log N)$ time.
    Square loss and best case data were used (integers from 1 to $N$, result in equal splits).
  }
\end{figure}

\begin{figure}[t!]
  \centering
  %\includegraphics[width=\textwidth]{figure-heap-refs-other}
  \input{figure-heap-refs-other}
  \caption{\label{fig:heap-refs}
    Estimating best-case asymptotic complexity for time (seconds) and memory (kilobytes).
    Black curves are empirical measurements for \pkg{changepoint} and \pkg{ruptures}, and violet curves are references.
    These data suggest that \pkg{changepoint} is $O(N^2)$ space and $O(N^3)$ time;
    \pkg{ruptures} is $O(N)$ space and $\approx O(N^{1.4})$ time.
    Square loss and best case data were used (integers from 1 to $N$, result in equal splits).
  }
\end{figure}
  
  

\end{appendix}


\end{document}
