\documentclass[article]{jss}
\usepackage[utf8]{inputenc}
%% -- LaTeX packages and custom commands ---------------------------------------
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\, min}
%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern,amsmath,amssymb}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 80, useFancyQuotes = FALSE)
library("MASS")
@


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation (and optionally ORCID link)
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Toby Dylan Hocking~\orcid{0000-0002-3146-0865}\\Université de Sherbrooke}
\Plainauthor{Toby Dylan Hocking}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{Comparing \pkg{binsegRcpp} with other implementations of binary segmentation for change-point detection}
\Plaintitle{Comparing binsegRcpp with other implementations of binary segmentation for change-point detection}
\Shorttitle{binsegRcpp for efficient change-point detection}

%% - \Abstract{} almost as usual
\Abstract{ Binary segmentation is a classic algorithm for detecting
  change-points in sequential data. In theory, using a simple loss
  function like Normal or Poisson, binary segmentation should be
  extremely fast for $N$ data and $S$ segments: asymptotically $O(N
  S)$ time in the worst case, and $O(N \log S)$ time in the best case.
  In practice, existing implementations can be asymptotically slower,
  and can return incorrect results.  We propose \pkg{binsegRcpp}, an
  \proglang{R} package which provides a correct \proglang{C++}
  implementation, with the expected asymptotic time complexity.
  %% In practice, existing
  %% implementations such as \pkg{ruptures} (\proglang{Python} module) and
  %% \pkg{changepoint} (\proglang{R} package) are asymptotically slower.
  %% Additionally, \pkg{changepoint} sometimes returns incorrect results.
  %% We present the \proglang{R} package \pkg{binsegRcpp}, which provides an efficient
  %% and correct C++ implementation of binary segmentation.
  We discuss several important \proglang{C++} coding techniques,
  and include detailed comparisons with other implementations of binary
  segmentation:
  \pkg{ruptures}, \pkg{fpop}, \pkg{changepoint}, \pkg{blockcpd}, and
  \pkg{wbs}.
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{\proglang{C++}, \proglang{R}, binary segmentation, change-point}
\Plainkeywords{C++, R, binary segmentation, change-point}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Toby Dylan Hocking\\
  Université de Sherbrooke\\
  E-mail: \email{Toby.Hocking@R-project.org}
}

\begin{document}


\section{Introduction: previous software for change-point detection} \label{sec:intro}

Change-point detection is an important problem in sequential data
analysis, that occurs in a variety of different applications, such as
genomics \citep{Hocking2013bioinformatics}, neuroscience
\citep{Jewell2019biostatistics}, and medical monitoring
\citep{Fotoohinasab2020segmentationEMBC}, among others.
The focus of
this article is change-point detection in the off-line setting, in
which we assume there is a sequence of $N$ observed data, and we want
an algorithm to identify the precise locations of any abrupt changes
that may be present \citep{Truong2020}. We assume there is
user-specified maximum number of segments $S$, and we would like
to compute a sequence of models from 1 to $S$ segments (0 to $S-1$ change-points).

Using
the classical dynamic programming algorithm of
\citet{segment-neighborhood}, optimal change-points for a given cost
function, and a sequence of model sizes $k\in\{1,\dots,S\}$, can be
computed in $O(N^2 S)$ time.
%\citep{Jackson2005}
More recently, the pruned dynamic programming algorithms of
\citet{Maidstone2017} can be used to compute the same sequence of $S$
optimal models, with the same $O(N^2 S)$ worst case time complexity,
but faster $O(NS)$ time best case time complexity.  Other dynamic
programming algorithms such as PELT \citep{pelt} and DuST
\citep{truong2024efficient} compute a single model in best case linear
time $O(N)$, worst case quadratic time $O(N^2)$.  They may be used as
a sub-routine to efficiently compute a range of $S$ change-point
models, using the CROCS algorithm \citep{Liehrmann2021chipseq}, which
again results in best case $O(N S)$ time, worst case $O(N^2 S)$ time.
\citet{fastcpd2024} propose a change-point algorithm based on dynamic programming, and provide comparisons with several previous algorithms.

Binary segmentation is a heuristic algorithm for computing a sequence of models from 1 to $S$ change-points \citep{binary-segmentation}.
Using a simple loss function like the square loss, the best case of binary segmentation is $O(N \log S)$ time \citep{Hocking2024finite}, which is much faster than dynamic programming.
The worst case of binary segmentation is $O(N S)$ time, which is the same speed as the best case of dynamic programming.



\begin{table}[t!]
  \centering
  \small
\begin{tabular}{l|cccccc}
%   > ruptures$version$version
% [1] "1.1.6"
% > sapply(c("binsegRcpp", "changepoint", "wbs", "fpop"), function(p)paste(packageVersion(p)))
%  binsegRcpp changepoint         wbs        fpop 
% "2022.3.29"     "2.2.3"       "1.4" "2019.8.26"
% https://github.com/HaotianXu/changepoints does not have binary segmentation.
  package & binsegRcpp & changepoint & wbs & fpop & ruptures & blockcpd\\
  function & binseg & cpt.mean & sbs & multiBinSeg & Binseg & fit\_blockcpd\\
  version & 2022.3.29 & 2.2.3 & 1.4 & 2019.8.26 & 1.1.9 & 1.0.0\\
\hline
weights & yes & no & no & no & no & no\\
min len & yes & yes & no & no & yes & yes\\
max segs & yes & yes & no & yes & yes & yes\\
dim & one & one & one & multi & multi & multi \\
correct & yes & no & yes & yes & yes & yes\\
losses & 5 & 6 & 1 & 1 & 10 & 5\\
language & C++ & C & C & C++ & Python & C++\\
% all propose some loss computation.
storage & heap & arrays & recursion & heap & LRU cache & heap \\
space & $O(S)$ & $O(S^2)$ & $O(S)$ & $O(S)$ & $O(S)$ & $O(S)$ \\
cumsum & yes & yes & yes & yes & no & yes\\
best & $O(N\log N)$ & $O(N^3)$ & $O(N\log N)$ & $O(N\log N)$ & $O(N^{1.4})$ & $O(N\log N)$\\
worst & $O(N^2)$ & $O(N^3)$ & $O(N^2)$ & $O(N^2)$ & $O(N^2)$ & $O(N^2)$\\
CV & yes & no & no & no & no & no\\
params & all & one & no & no & no &one\\
\end{tabular}
\caption{\label{tab:packages} Properties (rows) of six binary
  segmentation libraries (columns).  Properties are weights (can
  specify observation-specific positive weights for the loss), min len
  (can specify minimum segment length parameter), max segs (can
  specify maximum number of segments parameter), dim (dimensions of
  input data), correct (does code return correct model parameters?),
  losses (number of loss functions implemented), language (programming
  language to implement loss), storage (technique used to store cost
  of splitting a segment), space (space complexity in number of
  segments $S$), cumsum (cumulative sum used), best and worst case
  time complexity (using $N$ data and $S=N$ segments), CV (support for
  cross-validation), params (parameters returned, one or all model
  sizes). }
\end{table}

\subsection{Existing implementations of binary segmentation}
There are several free/open-source implementations of binary
segmentation, most of which are \proglang{R} packages with
\proglang{C}/\proglang{C++} code (Table~\ref{tab:packages}).  We
compared our proposed implementation to these baselines in terms of
functionality, correctness, and efficiency.  None of these existing
implementations support cross-validation, nor observation-specific
weights, which are unique features that we propose in \pkg{binsegRcpp}.

The \pkg{changepoint} package in \proglang{R} contains the
\code{cpt.mean} function which implements several
different change-point detection algorithms, including binary
segmentation via \code{method="BinSeg"}
\citep{Killick2014,Killick2022}.  Six loss functions are supported,
along with min segment length parameter, max segments parameter, and
returning the segment-specific parameters for one model (not all).
The
implementation does not always return correct results, and requires
more time/space than expected. For $S$ segments, an $S\times S$ matrix
is used to represent the segment end positions, whereas only a vector
of $S$ values is required. Therefore, the time complexity is $O(N
S^2)$, much slower than the expected $O(N\log S)$. Likewise, the space
complexity is $O(S^2)$ rather than the expected $O(S)$.

The \pkg{wbs} package in \proglang{R} contains the \code{sbs} function,
which implements the square loss for univariate data
\citep{Baranowski2019}. Since it uses recursive function calls, it
does not support the max segments parameter $S$, and instead always
computes the full path of models (from 1 to $N-1$ change-points).
It does not implement min segment length, nor
returning the segment mean parameters.

The \pkg{fpop} package in \proglang{R} contains the \code{multiBinSeg}
function, which implements the square loss for multi-dimensional data
\citep{Maidstone2017}. It implements the max segments parameter, but
not min segment length, nor
returning the segment mean parameters.

The \pkg{ruptures} package in \proglang{python} contains the
\code{Binseg} class, which implements binary segmentation for 10 loss
functions \citep{Truong2020}. It supports min length parameter and max
segments parameter. The cumulative sum is not used in the
implementation of the square loss, which is normally required to
achieve optimal best case time complexity. When running on $N$ best
case data, with $S=N$ segments, we have observed approximately
$O(N^{1.4})$ time complexity (see Results section), which is slower
than the expected/optimal $O(N\log N)$ time complexity.

The \pkg{blockcpd} package in \proglang{R} contains the
\code{fit\_blockcpd} function, which implements several algorithms for
change-point detection, including binary segmentation via
\code{method="hierseg"} \citep{blockcpd2021,Prates2021}. It supports
input of min segment length and maximum segments, and output of
segment mean parameters for one model size (not all).

\section{Statistical model, distributions and loss functions}

We assume there is a sequence of $N$ data $x_1,\dots,x_N\in\mathbb R$,
in which we want to find abrupt changes.  For each index
$i\in\{1,\dots,N\}$, we assume $x_i\sim\mathcal D(\theta_i)$, meaning
the observed data value $x_i$ is drawn from some distribution
$\mathcal D$ with un-observed parameter $\theta_i$, typically the mean
(and perhaps variance/scale).  We would like to compute a sequence of
parameter values, $\theta_1,\dots,\theta_N$, which is piecewise
constant.  We would like to find a sequence of change-points, such
that each segment has a parameter $\theta$ which minimizes a loss
function $\ell(\theta,x)$ that depends on the distribution $\mathcal
D$. The loss functions that we consider to minimize can be interpreted
as maximizing the log likelihood of several common distributions
(Normal, Poisson, Laplace).

Furthermore, we assume that each data point $i\in\{1,\dots,N\}$ has a
corresponding weight $w_i>0$, which is used in the loss function via
$w_i \ell(\theta, x_i)$. For example, minimizing the square loss $w_i
\ell(\theta, x_i) = w_i (\theta-x_i)^2$ corresponds to maximizing the
normal log-likelihood, when the variance is uniform.

\paragraph{Choice of observation-specific weights.}
Weights can be used to efficiently handle two particular kinds of data.
First, in the case of data which are sampled uniformly across
space/time, typically the weights are constant ($w_i=1$ for all $i$),
but other weights can be used for data which are sampled non-uniformly
(for example daily time series with some missing days).
Second, in the case of data with runs of identical values, a run-length encoding can be used to convert the raw/unweighted data sequence into a compressed/weighted data sequence that can be more efficiently processed.
For example, in genomics, DNA sequence reads are aligned to a reference genome, and a coverage profile is computed by counting the number of aligned reads at each reference position.
Since each DNA sequence read is about 100 bases long, the coverage profile has runs of identical values, which can be converted to a run-length encoding for $\approx 100\times$ speedups.

We consider five change-point models with loss functions $\ell$, each
of which have a corresponding value for the \code{distribution.str}
argument of the proposed function, \code{binsegRcpp::binseg}
(Table~\ref{tab:losses}).  The five models can be divided in two
categories: absolute error and cumulative sum.

\subsection{Loss functions based on absolute error}

Two models have loss functions that involve the absolute error
function, and the Laplace distribution. The probability density
function of the Laplace distribution, for data $x\in\mathbb R$, with
location parameter $\mu\in\mathbb R$, and scale parameter $b>0$, is
\begin{equation}
  \frac{1}{2b} \exp\left(
  \frac{|\mu-x|}{-b}
  \right).
\end{equation}
For \verb|distribution.str="l1"|, the segment-specific parameter
$\theta=\mu\in\mathbb R$ is the median. In this case, maximizing the
probability corresponds to minimizing the negative log likelihood,
which simplifies to the absolute error loss function,
\begin{equation}
  \ell_{\texttt{l1}}(\mu, x) = |\mu-x|.
\end{equation}

For \verb|distribution.str="laplace"|, the segment-specific parameter
$\theta=[\mu,b]\in\mathbb R^2$ is a vector with two values, the median
$\mu$ and scale $b$. In this case, the loss function we use is the
negative log likelihood,
\begin{equation}
  \ell_{\texttt{laplace}}([\mu,b],x)=\log(2b) + |\mu-x|/b.
\end{equation}
For a segment from $j$ to $e$ ($1\leq j\leq e\leq N$), the estimated parameters are
\begin{eqnarray}
  \mu_{j,e}^* &=& \argmin_\mu \sum_{i=j}^e w_i |\mu-x_i| \\
  b_{j,e}^* &=& \frac{\sum_{i=j}^e w_i |\mu^*-x_i|}{\sum_{i=j}^e w_i}.
\end{eqnarray}
Note that there could be a range of $\mu_{j,e}^*$ values which minimize the absolute error (for example with an even number of data and uniform weights). In that case, we define $\mu_{j,e}^*$ as the value in the middle of the range of values which minimize the absolute error (this is the traditional definition of the sample median).

%% using the cumulative median
%% For loss functions which require the cumulative median, we use the C++ code below
%% The loss function which requires a variance estimate is the Laplace negative log likelihood; the simple L1 loss is obtained without a variance estimate.
In the proposed C++ code, a \code{class absDistribution} implements
the common operations for the \texttt{l1} and \texttt{laplace} loss
functions, including parameter estimation via a cumulative median
algorithm, which is required to efficiently compute the optimal split
point on a segment.  In particular, when we search for the best split
on a segment with $s=e-j+1$ data, we need to compute the sequence of
parameter estimates $\mu^*_{j,j},\dots,\mu^*_{j,e-1}$ before the
split, and $\mu^*_{j+1,e},\dots,\mu^*_{e,e}$ after the split.
Computing these parameter estimates and corresponding loss values can
be done in $O(s\log s)$ time, using a modified version of the L1 loss
minimization code proposed by \citet{Drouin2017mmit}.

\subsection{Loss functions based on cumulative sums}

There are three models which use the cumulative sum trick to find the
best split on a segment with $s$ data in $O(s)$ time.
In particular, for a sequence of $N$ data, we compute the vectors $W,X,Y\in\mathbb R^{N+1}$ in linear $O(N)$ time via:
\begin{eqnarray}
  \text{Cumulative weights: } W_0=0,\,&& \forall t\in\{1,\dots,N\},\, W_t=\sum_{i=1}^t w_i = W_{t-1}+w_i,\\
  \text{Cumulative sum: }X_0=0,\,&& \forall t\in\{1,\dots,N\},\, X_t=\sum_{i=1}^t w_i x_i=X_{t-1}+w_i x_i,\\
  \text{Cumulative squares: }Y_0=0,\,&& \forall t\in\{1,\dots,N\},\, Y_t=\sum_{i=1}^t w_i x^2_i=Y_{t-1}+w_i x_i^2.
\end{eqnarray}


\begin{table*}
  \centering
  \begin{tabular}{ccccccc}
    \code{distribution.str} & Loss & Distrib. & Center & Scale & Params & Complexity \\
    \hline
    \texttt{mean\_norm} & Square & Normal & yes & no & 1 & $O(N)$ \\
    \texttt{meanvar\_norm} & Neg. Log Lik. & Normal & yes & yes & 2 & $O(N)$ \\
    \texttt{poisson} & Neg. Log Lik. & Poisson & yes & yes & 1 & $O(N)$ \\
    \texttt{l1} & Absolute & Laplace & yes & no & 1 & $O(N\log N)$ \\
    \texttt{laplace} & Neg. Log Lik & Laplace & yes & yes & 2 & $O(N\log N)$ \\
  \end{tabular}
  \caption{\label{tab:losses} Different change-point models provided
    by \code{binsegRcpp::binseg}.  All models support a change in
    center parameter; two models have an additional scale parameter;
    change in Poisson rate parameter affects both mean and variance.
    Complexity is time for computing best split on a segment with $N$ data.
  }
\end{table*}

For \verb|distribution.str="mean_norm"|, the normal change in mean model,
the segment-specific parameter $\theta\in\mathbb R$ is the mean.
For data $x\in\mathbb R$, mean $\mu\in\mathbb R$, and variance $\sigma^2>0$, the normal probability density function is
\begin{equation}
  \frac{1}{\sqrt{2\pi\sigma^2}}
  \exp\left[
    \frac{
      (\mu-x)^2
    }{
      -2\sigma^2
    }
    \right].
\end{equation}
Maximizing the normal probability is equivalent to minimizing the negative log-likelihood, which simplifies to the square loss,
\begin{equation}
  \ell_{\texttt{mean\_norm}}(\mu, x) = (\mu-x)^2.
\end{equation}
For a segment from $j$ to $e$ ($1\leq j\leq e\leq N$), we use the notation $W_{j,e}=W_j-W_{e-1}$ for the sum of the weights over the segment, and similarly for $X_{j,e}$ and $Y_{j,e}$. Then the mean on the segment from $j$ to $e$ can be computed in constant $O(1)$ time via
\begin{equation}
  \label{eq:mu-star}
  \mu^* = X_{j,e}/W_{j,e}.
\end{equation}

For \verb|distribution.str="meanvar_norm"|, the normal change in mean
and variance model, the segment-specific parameter $\theta=[\mu,\sigma^2]\in\mathbb R^2$ is a vector with two values, the mean $\mu$ and variance $\sigma^2$. The Gaussian negative log likelihood is used as the loss function,
\begin{equation}
\ell_{\texttt{meanvar\_norm}}([\mu,\sigma^2],x)=[\log(2\pi\sigma^2)+(\mu-x)^2/\sigma^2]/2.
\end{equation}
For a segment from $j$ to $e$ ($1\leq j\leq e\leq N$), the variance estimate is computed in constant $O(1)$ time via
\begin{equation}
  \sigma^{2*}=Y_{j,e}/W_{j,e} -\mu^*(\mu^*-2X_{j,e}/W_{j,e}).
\end{equation}
%% TODO discuss max zero var, which is the largest value that should be
%% considered numerically zero for a variance estimate. \code{INFINITY}
%% is returned for the loss if the estimated variance is smaller than max
%% zero var.

For \verb|distribution.str="poisson"|, the Poisson model, the
segment-specific parameter $\theta=\mu\geq 0$ is the rate.  The Poisson
probability mass function for a data value $x\in\{0,1,2,\dots\}$ is
\begin{equation}
  \mu^x e^{-\mu} / (x!).
\end{equation}
Maximizing the probability is equivalent to minimizing the negative
log likelihood, which simplifies to the Poisson loss,
\begin{equation}
  \ell_{\texttt{poisson}}(\mu,x)=\mu - x\log \mu.
\end{equation}
The Poisson rate parameter $\mu$ is estimated via the sample mean~(\ref{eq:mu-star}).



%% Another C macro is used to declare the loss functions which require the cumulative sum:
%% \begin{verbatim}
%% #define CUM_DIST(NAME, COMPUTE, ERROR, VARIANCE)...
%% CUM_DIST(mean_norm, RSS, , false)
%% CUM_DIST(meanvar_norm,
%%  (var>max_zero_var) ? (RSS/var+N*log(2*M_PI*var))/2 : INFINITY,
%%  , true)
%% CUM_DIST(poisson,
%%  (mean>0) ? (mean*N - log(mean)*sum) : ( (sum==0) ? 0 : INFINITY ),
%%  if(round(value)!=value)return ERROR_DATA_MUST_BE_INTEGER_FOR_POISSON_LOSS;
%%  if(value < 0)return ERROR_DATA_MUST_BE_NON_NEGATIVE_FOR_POISSON_LOSS;,
%%  false)
%% \end{verbatim}
%% The code above declares three loss functions based on the cumulative sum: \code{mean_norm} is the residual sum of squares (RSS), \code{poisson} loss is for count data, and \code{meanvar_norm} is the normal model with change in mean and variance.
%% The \code{COMPUTE} parameter specifies the loss, in terms of six variables: \code{mean}, \code{var}, \code{N}, \code{sum}, \code{squares}, \code{max_zero_var} (Table~\ref{tab:cum-params}).
%% These variables are computed in a \code{class CumDistribution} which contains the logic common to these loss functions (cumulative sum, etc).


%% To define the loss functions based on the cumulative sum TODO
%% \begin{table}
%%   \begin{tabular}{ccc}
%%     Parameter & Description & Definition \\
%%     \hline
%%     $M=$\code{mean} & model parameter & $T/W$ \\
%%     \code{var} & model parameter &  $S/W + M(M-2T/W)$
%%   \end{tabular}
%%   \caption{\label{tab:params}TODO}
%% \end{table}

%% Additionally, the \code{CUM_DIST} and \code{ABS_DIST} macros generate code that populates a static \code{unordered_map} which can be queried to obtain a list of distributions which are supported by the C++ code:
%% %
%% <<>>=
%% binsegRcpp::get_distribution_info()$dist
%% @ 
%% %


\section{Algorithm} \label{sec:algorithm}
This section gives details of the binary segmentation algorithm.

\subsection{Efficient computation of optimal split point on a segment}
The iterative algorithm involves computing the min loss over
candidate segments, each of which is defined by a start $j$ and end
$e$ position ($1<j<e<N$).

\begin{equation}
L_{j,e} = \min_{\theta} \sum_{i=j}^e w_i \ell(\theta, x_i).
\end{equation}
The minimization problem is over the model parameter $\theta$.

\paragraph{Best loss difference.}
Since different segments have different loss values before and after splitting, the algorithm must choose the next segment to split by minimizing a loss difference value.
Let $D_{j,e}(t)=L_{j,t}+L_{t+1,n}-L_{j,e}<0$ be the loss difference after splitting segment $(j,e)$ after $t$.
The $L_{j,e}$ term is the loss before the split, and $L_{j,t}+L_{t+1,n}$ is the loss after the split, which is necessarily smaller.
We define the best loss difference/split on segment $(j,e)$ as
\begin{equation}
  \label{eq:fje}
  f_{j,e}=\min,\argmin_{t\in \{j,\dots,e-1\}} D_{j,e}(t).
\end{equation}
%% We can re-write this minimization as
%% \begin{equation}
%%   f_{j,e}=\min,\argmin -L_{j,e}+ \begin{cases}
%%     D_{j,e}(j) = L_{j,j}+L_{j+1,e} \\
%%     %D_{j,e}(j+1) = L_{j,j+1}+L_{j+2,e} \\
%%     \vdots\\
%%     D_{j,e}(e-1) = L_{j,e-1}+L_{e,e} \\
%%   \end{cases}
%%   \label{eq:fje}
%% \end{equation}
\paragraph{Efficient computation of optimal split point.}
To compute the loss differences $D_{j,e}(j),\dots,D_{j,e}(e-1)$, we need to compute the sequence of values $L_{j,j}, \dots, L_{j,e-1}$ (loss values for possible new segments before candidate split points), and the sequence of values $L_{j+1,e}, \dots, L_{e,e}$ (loss values for possible new segments after candidate split points).
These sequences can be computed efficiently for the five loss functions that we implemented in \code{binsegRcpp}.
The time complexity of computing these sequences depends on the loss function, and the size of the segment to split, $s=e-j+1$.
For the case of negative log likelihood of Normal and Poisson distributions, the cumulative sum trick can be used to compute these sequences in linear $O(s)$ time.
For the case of the L1 loss and negative log likelihood of Laplace distribution, these sequences can be computed efficiently in log-linear $O(s\log s)$ time using a cumulative median algorithm.
To implement the cumulative median, we used a modified version of the L1 loss minimization code proposed by \citet{Drouin2017mmit}.

\subsection{Containers for efficient retreival of next segment to split}
\label{sec:data-structures}
The binary segmentation algorithm needs to keep track of a set of segments that can be split, $\mathcal S_k$ for iteration $k$.
Each segment $(j,e,d,t)\in\mathcal S_k$ is represented by the start/end positions $(j,e)$ along with the corresponding min loss difference $d<0$, as well as the best split point $t\in\{j,\dots,e-1\}$.

\begin{table}
  \centering
  \begin{tabular}{c|cc|cc|cc}
    Splits: & \multicolumn{2}{c|}{one split, iteration $k$} & \multicolumn{2}{c|}{$S$ equal splits} & \multicolumn{2}{c}{$S$ unequal splits} \\
    operation: & insert &
    %remove &
    argmin & insert & argmin & insert & argmin\\
    \hline
    list & $O(1)$ &
    % $O(1)$ &
    $O(k)$ &
    $O(S)$ & $O(S^2)$ &
    $O(S)$ & $O(S)$\\
    \hline
    heap/multiset & $O(\log k)$ &
    % $O(\log N)$ &
    $O(1)$ &
    $O(S\log S)$ & $O(S)$ &
    $O(S)$ & $O(S)$\\
    \hline
    Compute loss & \multicolumn{2}{c|}{---} & \multicolumn{2}{c|}{$O(N\log S)$} & \multicolumn{2}{c}{$O(NS)$}
  \end{tabular}
  \caption{
    Asymptotic time complexity for container operations (insert new segment which could be split, argmin to find best segment to split next).
    At iteration $k$, list takes $O(k)$ time, whereas heap/multiset takes $O(\log k)$ time.
    Best case time for finding $S$ splits in $N$ data is $O(N\log S)$,
    which can be achieved by heap/multiset, because insert $O(S\log S)$ time is faster.
    However, best case time can not be achieved by list, because argmin $O(S^2)$ time is slower.
  }
  \label{tab:containers}
\end{table}
In binary segmentation, a segment is split in each iteration, which results in up to two new segments that must be considered to split. % (\ref{eq:Nk}).
Computing the cost of splitting a segment of size $s$ is $O(s\log s)$ time for L1/Laplace losses, and $O(s)$ time for the normal/Poisson losses.
After computing the cost for two new segments, the cost of all splittable segments must be considered, in order to identify the min cost segment to split next.
We propose using three different kinds of C++ Standard Template Library containers to represent $\mathcal S_k$ (set of splittable segments at iteration $k$).
Each container has a different implementation of storage and retreival of the cost of segments which have previously been considered, but not yet split (Table~\ref{tab:containers}).
First, we propose using the \code{multiset} container (red-black tree) and \code{priority\_queue} container (heap), which can be used to achieve best case time complexity.
These containers keep segments sorted by their cost values, so offer constant $O(1)$ time retreival of the min cost segment to split next; insertion takes logarithmic $O(\log n)$ time in the number of segments $n$ currently stored in the container.

\paragraph{List container as baseline.}
Second, we propose using the \code{list} container (doubly-linked list), which is used as a baseline, to demonstrate the benefits of the other containers.
The \code{list} keeps segments stored in the order in which they were inserted, so offers constant $O(1)$ time insertion of new elements, and linear $O(n)$ time retreival of the min cost segment to split, for a container with $n$ segments currently stored.
In the best case run-time of binary segmentation, there are equal sized splits at each iteration, with two new segments that could be split, but only one that is split (and the other one which stays in the set $\mathcal S_k$).
Therefore the best case size of the set of splittable segments at iteration $k$ is linear, $|\mathcal S_k|=O(k)$, and the \code{list} container therefore takes $O(k)$ time to find the next segment to split.
Going up to $S$ segments, the list therefore takes $O(S^2)$ time, which can dominate the overall time for large $S$, and prevent the code from achieving the best case $O(N \log S)$ time.

\subsection{Algorithm pseudo-code}
This section gives the update rules for the binary segmentation algorithm.
\paragraph{Initialization.}
The initialization of the algorithm is as follows.
Let $\mathcal L_1=L_{1,n}$ be the loss with one segment,
and let $ f_{1,n}$ be the best loss difference and split point for the entire data.
We initialize the set of segments to split as $\mathcal S_1=\{(1,N,f_{1,n})\}$.



\paragraph{Recursive update rules.}
Let $S\in\{ 2, \dots, N\}$ be the user-specified max number of segments to consider.
The binary segmentation algorithm recursively computes the following quantities for each iteration $k\in\{2,\dots,S\}$.
First, the best segment to split is computed by greedy minimization of the loss decrease over all splittable segments.
\begin{equation}
  j^*_k, e^*_k, d^*_k, t^*_k = \argmin_{(j,e,d,t)\in\mathcal S_{k-1}} d.
\end{equation}
Ideally, the minimization above takes $O(1)$ constant time (independent of the number of segments that could be split), if the set
of splittable segments is stored in an efficient container (for example, C++ Standard Template Library multiset or priority\_queue). As a
baseline, we also consider a linked list, which results in a
sub-optimal $O(k)$ linear time search at iteration $k$ (see
Section~\ref{sec:data-structures} for details).

In the second update at iteration $k$, the total loss is computed by adding the best loss decrease $d^*_k$ to the best loss $L_{k-1}$ at the previous iteration, 
\begin{equation}
  \mathcal L_k=\mathcal L_{k-1} +d^*_k.
\end{equation}

Third, we construct a set of two new segments to consider:
\begin{equation}
  \label{eq:Nk}
  \mathcal N_k=\{
  (j^*_k, t^*_k),
  (t^*_k+1,e^*_k)
  \}.
\end{equation}
New segments in $N_k$ could be too small to split, if there is only one data point on that segment.
We therefore consider only the segments that are large enough to be split:
\begin{equation}
  \label{eq:Vk}
  \mathcal V_k=\{(j,e,f_{j,e})\mid (j,e)\in\mathcal N_k,\, j<e\}.
\end{equation}

Finally, we update the set of splittable segments, by removing the currently split segment, and inserting the new segments that could be split:
\begin{equation}
  \mathcal S_k=
[\mathcal S_{k-1}\setminus (j^*_k, e^*_k, d^*_k, t^*_k)]
\cup\mathcal V_k.
\end{equation}
For an efficient implementation, the time complexity of these
remove/insert operations should be sub-linear in the container size,
which is true of all three containers that we consider. The list container
offers constant time insertion/removal; the multiset and
priority\_queue containers offer constant time removal and log-time
insertion.

\subsection{Modification for min segment length parameter}

Sometimes there is prior knowledge that there should be no segments
with fewer data points than $m\in\{1,2,\dots\}$. In that case, $m$ is
referred to as a min segment length parameter, and there are two simple
modifications to the equations above.
First, the best split (\ref{eq:fje}) is re-defined as
\begin{equation}
f^m_{j,e}=\min,\argmin_{t\in\{
j+m-1,\dots,e-m
\}} D_{j,e}(t).
\end{equation}
Second, the set of splittable segments (\ref{eq:Vk}) is re-defined as
\begin{equation}
  \mathcal V^m_k=\{(j,e,f_{j,e})\mid (j,e)\in\mathcal N_k,\, 
  e-j+1\geq 2m
  \}.
\end{equation}

\section{Demonstration and comparison of code}
In this section, we show example code for computing binary segmentation models using several software packages.
We aim to emphasize how \pkg{binsegRcpp} makes it easy to retreive segment-specific model parameters.
To simplify the discussion in this section, we use ``segment mean'' to represent arbitrary segment-specific model parameters.
For example, segment-specific model parameters can be the mean/variance of the normal distribution, or median/scale of the Laplace distribution.

A unique feature that is provided by \pkg{binsegRcpp}, but not provided by other implementations of binary segmentation, is efficient storage and retrieval of arbitrary segment mean parameters.
In contrast, \code{changepoint::cpt.mean} returns only the segment means for the selected model, and other implementations do not return any segment means (but the returned change-point variables could be used to compute estimates).

%data_vec <- c(5, 0, 20, 15, -10, 0)
For example, we consider a simple example with six data points.
<<>>=
ex6_df <- data.frame(value = c(1, -7, 8, 10, 2, 4))
@
The code below can be used to compute models up to a max of 4 segments, using the square loss (normal change in mean, constant variance).
<<>>=
ex6_binsegRcpp <- binsegRcpp::binseg(
  "mean_norm", ex6_df$value, max.segments = 4)
ex6_binsegRcpp$splits[, .(segments, end, before.mean, after.mean,
  invalidates.index, invalidates.after)]
@
The output above is a table with four rows, one for each model size.
For a max number of segments $S$, this representation of model parameters uses $S$ rows, so is asymptotically linear space, $O(S)$.
The first row represents the trivial model: one segment across all data points, with \code{end=6} equal to the index of the last data point.
The model parameter of the trivial model is \code{before.mean=3}, which is the mean of all of the data.
The other rows represent the segments that result from the recursive splitting algorithm.
For example, in the second row, \code{end=2} means that the first change-point results in a split between data points 2 and 3.
The \code{before.mean = -3} is the mean up to data point 2, and \code{after.mean=6} is the mean from data point 3 to the end.
The other columns, \code{invalidates.index} and \code{invalidates.after}, indicate the previous means which are no longer valid, for models with the given split.
For example, in the second row, \code{invalidates.index=1} identifies the first row, and \code{invalidates.after=0} identifies the mean before the split (not after), which implies that \code{before.mean=3} is no longer valid, after performing the split in the second row.
In the third row, we see a new split at \code{end=4}, resulting in new parameters \code{before.mean=9} and \code{after.mean=3}, which invalidate the previous mean in row 2 after the split (\code{after.mean=6}), while the other mean in row 2 remains valid (\code{before.mean = -3}).

In general, after running binary segmentation up to $S$ segments, the model parameters are stored in a table with $S$ rows.
For any number of segments $k\in\{1,\dots,S\}$, we can compute a table of $k$ estimated means (one row for each segment), using the following algorithm.
First, consider only the first $k$ rows of the \code{splits} table.
Next, set values in \code{before.mean} and \code{after.mean} columns
to missing (\code{NA}), based on values in columns \code{invalidates.index}
and \code{invalidates.after}.  Finally, sort the \code{end}
values, and use the corresponding non-missing mean values.  For $k$
segments, this algorithm is $O(k \log k)$ time due to the sort.  It is
implemented in the \code{coef} method, as shown in the example code
below:

<<>>=
(ex6_segments <- coef(ex6_binsegRcpp, 2:4))
@

The table above contains one row per segment, for three model sizes (two to four segments).
It can be used to plot the segment means and change-point variables, using the code below.

<< ex6, fig=FALSE, height=3 >>=
library(ggplot2)
ex6_df$position <- 1:nrow(ex6_df)
ggplot() + geom_point(aes(position, value), data = ex6_df) +
  geom_segment(aes(start.pos, y = mean, xend = end.pos, yend = mean),
    color = "green", data = ex6_segments) +
  geom_vline(aes(xintercept = start.pos), color="green",
    linetype = "dashed", data = ex6_segments[1 < start]) +
  facet_grid(segments ~ ., labeller = label_both)
@
The code above results in Figure~\ref{fig:ex6}.


\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{article-ex6}
  \caption{\label{fig:ex6} Change-point models with 2 to 4 segments computed by \pkg{binsegRcpp}.
    Data points are shown in black, along with the model in green (dashed vertical change-points, and solid horizontal segment means).
    %and visualized using the mean parameters output via the \code{coef()} method.
  }
\end{figure}

\subsection{Comparison with changepoint}

To attempt the analogous computation with \pkg{changepoint}, we can use the code below.

<<>>=
ex6_changepoint <- changepoint::cpt.mean(ex6_df$value,
  method = "BinSeg", Q = 3, penalty = "Manual", pen.value = 0)
changepoint::cpts.full(ex6_changepoint)
@

The output above is the matrix of change-points, one row for each iteration of binary segmentation.
One issue with the output above is that the change-points are incorrect in rows 2 and 3.
For example, row 2 represents the model with two change-points, which appear as 0 and 2 in the matrix above.
However, for the model with two change-points, the expected result is segments that end at data points 2, 4, and 6.
These results indicate that the implementation of binary segmentation in \pkg{changepoint} can yield incorrect change-point detection results.
Incorrect results can also be observed in real data sets, such as in Figure~\ref{fig:neuroblastoma}, which shows that \pkg{changepoint} computes models with sub-optimal change-points and loss values.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{changepoint.bug.png}
  \caption{\label{fig:neuroblastoma} For a real cancer DNA copy number
    data set with 273 observations, we show the first four
    change-points models for two different R packages that provide code for
    binary segmentation: \pkg{binsegRcpp} and \pkg{changepoint}.
  }
\end{figure}

Another issue with \pkg{changepoint} is that the storage of change-points is relatively inefficient.
For a maximum number of change-points $Q$, it uses a $Q\times Q$ matrix of integers to represent the change-points, so the storage space is quadratic $O(Q^2)$.
In contrast, \pkg{binsegRcpp} uses only linear space, $O(Q)$, which is asymptotically more efficient.
Therefore, \pkg{changepoint} can only handle models with relatively small numbers of change-points, whereas \pkg{binsegRcpp} can efficiently store arbitrarily large change-point models.

A final issue is illustrated via the code below,
%rbind(changepoint = changepoint::param.est(ex6_changepoint)$mean,
%      expected = ex6_segments[segments==3, mean])
<<>>=
changepoint::param.est(ex6_changepoint)
@

The result above shows the estimated segment means, for which there are two issues.
First, there are only three values, so these should be the segment means for the model with two change-points.
However, these three values are incorrect (the correct segment means are -3, 9, and 3).
Another issue is that the segment means are only reported for one model size,
and it is not possible to retreive means for other model sizes.

\subsection{Comparison with wbs}

Like \pkg{binsegRcpp}, the \pkg{wbs} package provides a correct implementation of binary segmentation.
However, it is missing some key features, such as the ability to specify a max number of segments.
The \pkg{wbs} code below computes the full path of binary segmentation models.

<<>>=
ex6_wbs <- wbs::sbs(ex6_df$value)
data.table::data.table(ex6_wbs$res)[order(-min.th)]
@

The \code{res} table above is similar to the \code{splits} table from \pkg{binsegRcpp}.
It has one row per split in binary segmentation, so there are five rows in this example (because there are six data points).
The \code{cpt} column is the index of the last data point before the detected change-point, which is consistent with the \code{end} column from \pkg{binsegRcpp}.
The \pkg{wbs} package implements binary segmentation using recursive function calls in C (depth-first search, which means it is impossible to specify a max number of segments).
To compute the change-points for some number of change-points $Q$, the \code{res} table must be sorted, and only the top $Q$ rows be kept.
For example, the model with $Q=2$ change-points can be represented using the first two rows of the table above (change-points at 2 and 4).
However, the \pkg{wbs} package does not support retreival of segment-specific model parameters such as segment means.
If the user wants segment mean estimates, they must be computed using the returned change-points.

\subsection{Cross-validation}
A unique feature of \pkg{binsegRcpp} is efficient cross-validation, which can be useful for selecting the number of change-points.
The data input to \code{binseg()} is called the train set, which is divided into a subtrain set (used to compute model parameters) and a validation set (used to evaluate the computed model parameters).
The assignment of data to each set is controlled by the \code{is.validation.vec} argument, which must be a logical vector (same length as the data to segment).
For example, we can simulate a data set with two change-points:
<<>>=
set.seed(8)
two_changes <- c(rnorm(7, 1), rnorm(10, 3), rnorm(5, 0))
@
Then we can create a vector to indicate that every other data point should be assigned to either subtrain or validation:
<<>>=
(is.validation.vec <- rep(c(TRUE, FALSE), l=length(two_changes)))
@
Then we can run binary segmentation with cross-validation via the code below:
<<>>=
valid_fit <- binsegRcpp::binseg("mean_norm", two_changes, 
  is.validation.vec = is.validation.vec)
valid_fit
@
The output above includes a table with one row per model size, and a column for the validation loss, which is minimized at 3 segments (as expected).
The validation loss is computed as the total loss of the segment means with respect to the overlapping validation data points, as shown in Figure~\ref{fig:subtrain-validation}.

%% . To visualize the calculation, we can use the coef method to obtain a table of segment means, as in the code below.
%% <<>>=
%% (vsegs <- coef(valid_fit, 3L))
%% @
%% Then we create a table to represent the data sequence in the code below.
%% << two-changes, fig=TRUE, height=2 >>=
%% vdata <- data.table::data.table(value = two_changes,
%%  position = seq_along(two_changes), set = ifelse(
%%    is.validation.vec, "validation", "subtrain"))
%% valid_join <- vsegs[vdata, .(position, mean, value, set, segments), on=.(start.pos<position, end.pos>position)]
%% valid_join[, .(loss=sum((mean-value)^2)), by=.(set, segments)]
%% valid_fit$splits[3, .(segments, loss, validation.loss)]
%% gg <- ggplot() +
%%   theme_bw()+
%%   scale_fill_manual(values = c(subtrain = "black", validation = "white")) +
%%   scale_color_manual(values = c("validation loss" = "violet", "subtrain mean" = "green")) +
%%   geom_point(aes(position, value, fill = set), shape = 21, data = vdata) +
%%   geom_segment(aes(
%%     start.pos, mean, xend = end.pos, yend = mean, color=line),
%%     data = data.frame(vsegs, line="subtrain mean")) + 
%%     scale_x_continuous(breaks=1:22)+
%%   geom_segment(aes(
%%     position, mean,
%%     color=line,
%%     xend=position, yend=value),
%%     size=1,
%%     data=data.frame(valid_join[set=="validation"], line="validation loss"))+
%%   theme(panel.grid.minor=element_blank())
%% print(gg)
%% @
%% The data and model are visualized in Figure~\ref{fig:subtrain-validation}.
\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{article-two-changes}
  \caption{\label{fig:subtrain-validation}
    Change-point model parameters (segment means and change-point positions) are computed using the subtrain set; validation set is used for selecting the number of change-points.
  }
\end{figure}


%% <<>>=
%% @ 

%% If the user wants to do model selection several subtrain/validation splits with the normal loss, the 
%% <<>>=
%% cv_fit <- binsegRcpp::binseg_normal_cv(two_changes)
%% cv_fit$cv
%% @ 

%% A C++ virtual class \code{Container} is used to represent the methods that must be provided by either type of container: \code{insert}, \code{get_best}.
%% We used a C macro, \code{CMAKER}, to declare the two sub-classes:
%% \begin{verbatim}
%% #define CMAKER(CONTAINER, INSERT, GET_BEST)...
%% CMAKER(multiset, insert, segment_container.begin())
%% CMAKER(list, push_back, std::min_element(
%%  segment_container.begin(),segment_container.end()))
%% \end{verbatim}
%% The code above is expanded by the C pre-processor, to two declarations of \code{Container} sub-classes.

%To illustrate the performence benefits of using log-time  containers  were used to efficiently , virtual classes, static variables, function pointers, templates, macros).



%% \paragraph{Loss computation.} We run each algorithm on \code{N.data}
%% points up to \code{max.segments =
%%   max.changes+1}. The \code{binsegRcpp::binseg} result is a list which
%% contains a data table with \code{max.segments} rows and column
%% \code{loss} that is the square loss. The \code{changepoint::cpt.mean}
%% result is a list of class \code{cpt.range} with method \code{logLik}
%% which returns the square loss of one of the models. The \code{wbs::sbs}
%% result is a list which contains a data frame with \code{N.data-1} rows
%% (one for each possible split). The \code{fpop::multiBinSeg} result is a
%% list with element \code{J.est}, which is a vector of
%% \code{max.changes} square loss decrease values. The
%% \code{ruptures.Binseg.predict} result is a vector of segment ends for
%% one model size, which can be passed to \code{sum_of_costs} method to
%% compute the square loss.

%% Three packages have implemented the normal change in mean and variance
%% model. \code{binsegRcpp::binseg} loss values are the normal negative
%% log likelihood (NLL). The \code{changepoint::logLik} function returns two
%% times the NLL. The \code{ruptures.Binseg}
%% loss is related via this equation,

%% \begin{equation}
%%   \text{NLL} = (\text{rupturesLoss} + N[1+\log(2\pi)])/2
%% \end{equation}


\section{Empirical time complexity analysis} \label{sec:illustrations}
This section presents results from empirical timings of the proposed
\pkg{binsegRcpp} package, along with baselines.

\subsection{Best case time using multiset but not list container}
First, we wanted to empirically validate the expected time complexity
of the proposed C++ code using the Standard Template Library
containers (Table~\ref{tab:containers}).  We expected the multiset
container to achieve the best case $O(N\log S)$ time for $N$ data and
$S$ segments; we expected the list container to be slower, $O(S^2)$
time, even for best case data.  To test this expectation empirically,
we used \pkg{binsegRcpp} with synthetic data sequences of varying
sizes $N\in\{2,4,8,\dots\}$, and a maximum number of segments $S=N/2$.
For each $N$, the data sequence we used was $1,\dots,N$, which results
in equal splits with the square loss: one segment of
size $N$ split into two of size $N/2$, each of which is split into two
of size $N/4$, etc.
In this simulation, $S=O(N)$, so we expected time complexity of $O(N\log N)$ for the multiset, and $O(N^2)$ for the list.
In Figure~\ref{fig:heap-refs-binsegRcpp}, it is clear that the empirical timings using the list container closely align with the $O(N^2)$ reference line, whereas the empirical timings using the multiset container closely align with the $O(N\log N)$ reference line.
These data indicate that \pkg{binsegRcpp} with the multiset container indeed achieves the best case asymptotic time complexity, $O(N\log S)$.
Futhermore, these data indicate that the best case time complexity can not be attained, if list container is used.
In addition, Figure~\ref{fig:heap-refs-binsegRcpp} clearly shows the linear space complexity of both containers, $O(N)$.

\begin{figure}[t!]
  \centering
  %\includegraphics[width=\textwidth]{figure-heap-refs-binsegRcpp}
  \input{figure-heap-refs-binsegRcpp}
  \caption{\label{fig:heap-refs-binsegRcpp}
    Estimating best-case asymptotic complexity for time (seconds) and memory (kilobytes).
    Black curves are empirical measurements for \pkg{binsegRcpp} with two containers, and violet curves are references.
    These data suggest that both list and multiset containers are $O(N)$ space
    list is $O(N^2)$ time and multiset is $O(N\log N)$ time.
    Square loss and best case data were used (integers from 1 to $N$, result in equal splits).
  }
\end{figure}

\subsection{Other packages which do not achieve best case time}
Next, we wanted to empirically verify the best case asymptotic time
complexity of two baselines, \pkg{changepoint} and \pkg{ruptures}. We
expected that these baselines should have the asymptotically optimal
time complexity, $O(N\log S)$ (only different from \pkg{binsegRcpp} by
constant factors). To test this hypothesis, we ran these baselines in
the same computational experiment described in the previous paragraph
(square loss, best case data, $S=N/2$). We observed in Figure~\ref{fig:heap-refs-other} that
\pkg{changepoint} takes $O(N^2)$ space, which is a larger asymptotic complexity class than expected, $O(N)$.
Similarly, we observed in Figure~\ref{fig:heap-refs-other} that \pkg{changepoint} takes $O(N^3)$ time, which is again larger than the expected $O(N\log N)$ complexity.
We observed in Figure~\ref{fig:heap-refs-other} that \pkg{ruptures} empirical timings align well with a $O(N^{1.4})$ asymptotic reference, which is clearly larger than $O(N\log N)$, but smaller than $O(N^2)$.
Contrary to our expectations, these data indicate that the implementations of binary segmentation in both \pkg{changepoint} and \pkg{ruptures} do not achieve the best case asymptotic time complexity.

\begin{figure}[t!]
  \centering
  %\includegraphics[width=\textwidth]{figure-heap-refs-other}
  \input{figure-heap-refs-other}
  \caption{\label{fig:heap-refs-other}
    Estimating best-case asymptotic complexity for time (seconds) and memory (kilobytes).
    Black curves are empirical measurements for \pkg{changepoint} and \pkg{ruptures}, and violet curves are references.
    These data suggest that \pkg{changepoint} is $O(N^2)$ space and $O(N^3)$ time;
    \pkg{ruptures} is $O(N)$ space and $\approx O(N^{1.4})$ time.
    Square loss and best case data were used (integers from 1 to $N$, result in equal splits).
  }
\end{figure}
  
\subsection{Comparing timings and throughput with baselines}
In Figure~\ref{fig:heap-pred}, we show the previously described empirical timings (Figures~\ref{fig:heap-refs-binsegRcpp}--\ref{fig:heap-refs-other}) on the same axes, for comparison.
Whereas there are not very large timing differences for small data sizes (all work in less than one second for up to 1000 data), there are large differences for data sizez of several thousand data or larger.
We highlight the estimated throughput at 5 seconds, which is the data size $N$ that each package can handle in that time limit.
It can be seen that \pkg{binsegRcpp} has about the same timings, using either \code{multiset} or \code{priority\_queue} containers (as expected because they both offer log-time retreival of the next segment to split).
Also, \pkg{binsegRcpp} with \code{list} container is about 100$\times$ slower (as expected because of the linear time retreival of the next segment to split).
It can be seen that \pkg{ruptures} and \pkg{changepoint} are significantly slower, with about $1000\times$ smaller throughput than \pkg{binsegRcpp} (thousands versus millions of data possible in a time limit of 5 seconds).
These data indicate that for large data (several thousand or more), \pkg{binsegRcpp} is preferable for speed.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-heap-pred}
  \caption{\label{fig:heap-pred} Comparing best-case computation time of \pkg{binsegRcpp} with three containers, and two baselines: \pkg{changepoint} and \pkg{ruptures}.
    Square loss and best case data were used (integers from 1 to $N$, result in equal splits).
  }
\end{figure}

\subsection{Other packages which achieve best case time}
We conducted empirical timings on other implementations of binary
segmentation (\code{wbs::sbs}, \code{fpop::multiBinSeg}, 
\code{blockcpd::heap}).
In Figure~\ref{fig:timings}, we show timings for the square loss in three different data sets (best case, worst case, flat).
It is clear that \code{fpop::multiBinSeg} and \code{wbs::sbs} have very similar asymptotic timings as \pkg{binsegRcpp}.
In the worst case, all packages show the same slope (indicating quadratic time), except \pkg{changepoint} (which has a larger slope, indicating cubic time).

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings.png}
\caption{\label{fig:timings} Timings using square loss.}
\end{figure}

We also investigated the Poisson model (Figure~\ref{fig:timings-poisson}),
and the normal change in mean and variance model
(Figure~\ref{fig:timings-meanvar_norm}), which are implemented by
\code{blockcpd::heap}.  It is clear that \code{blockcpd::heap} has the
same asymptotic slope, but much larger constant factors, when compared
with \pkg{binsegRcpp}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings-meanvar_norm.png}
\caption{\label{fig:timings-meanvar_norm} Timings using Gaussian change in mean and variance model.}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings-poisson.png}
\caption{\label{fig:timings-poisson} Timings using Poisson loss.}
\end{figure}

\section{Summary and discussion} \label{sec:summary}

\begin{leftbar}
As usual \dots
\end{leftbar}

\bibliography{refs}

\newpage

\begin{appendix}

  \section{More empirical timings} \label{app:more-timings}

  We also performed computational experiments involving the L1 loss
  (Laplace change in median), which was implemented by \pkg{ruptures}.
  Figure~\ref{fig:timings-laplace} shows that \pkg{ruptures} has slightly larger asymptotic slope, and much larger constant factors, than \pkg{binsegRcpp} (in the best case).

  Figure~\ref{fig:compare-distributions} shows \pkg{binsegRcpp}
  timings for four loss functions, and three cases (best, worst,
  flat). It can be seen that the Laplace median model (L1 loss) has
  slightly larger asymptotic slope in the best and flat cases (but
  same quadratic slope in the worst case).

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-timings-laplace.png}
\caption{\label{fig:timings-laplace} Timings using L1 loss.}
\end{figure}


\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figure-compare-distributions.png}
  \caption{\label{fig:compare-distributions} Timings using binsegRcpp multiset with
    several different loss functions.}
\end{figure}
  

\end{appendix}


\end{document}
