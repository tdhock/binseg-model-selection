Experiments to support upcoming research papers about binary
segmentation.

** Paper: Finite sample complexity analysis of binary segmentation

To reproduce the analyses and figures in the paper you can use the code below:
- Figure 1: synthetic data which require tie-breaking. [[file:figure-synthetic.R][R code]], [[file:figure-synthetic.png][PNG
  figure]].
- Figure 2: optimal binary trees. [[file:figure-optimal-trees.R][R code]], [[file:figure-optimal-trees-some.png][PNG figure]].
- Figure 3: number of candidate splits considered in real genomic
  ChIP-seq data from McGill benchmark. [[file:figure-mcgill-iterations-data.R][R code 1]] makes [[file:figure-mcgill-iterations-data.csv][CSV file]], [[file:figure-mcgill-iterations.R][R code
  2]] makes [[file:figure-mcgill-iterations.png][PNG figure]].
- Figure 4: number of candidate splits considered in real genomic copy
  number data from neuroblastoma benchmark. [[file:figure-neuroblastoma-iterations-data.R][R code 1]] makes [[file:figure-neuroblastoma-iterations-data.csv][CSV file]], [[file:figure-neuroblastoma-iterations.R][R
  code 2]] makes [[file:figure-neuroblastoma-iterations.png][PNG figure]].

** TODOs

- Compare with Julia
  https://github.com/STOR-i/Changepoints.jl/blob/master/src/BS.jl
  maybe using
  https://cran.r-project.org/web/packages/JuliaCall/readme/README.html
- MATLAB https://www.mathworks.com/help/signal/ref/findchangepts.html#mw_748d5529-e05a-4b55-a3fe-2a12a5772d22
- NAG licensing and installation too complicated,
  https://www.nag.com/numeric/nl/nagdoc_latest/clhtml/g13/g13ndc.html,
  https://www.nag.com/content/calling-nag-fortran-routines-r,
  https://www.nag.com/content/nagfwrappers-r-package
- Try ruptures cumsum https://github.com/deepcharles/ruptures/issues/245#issuecomment-1112394892

** 2 May 2025

[[file:changepoint.bug.R]] makes figure for https://github.com/rkillick/changepoint/issues/86

[[file:changepoint.bug.png]]

** 7 Aug 2022 

[[file:figure-compare-ruptures.R]] makes the figure below which plots
asymptotic reference lines on top of the empirical median timing,

[[file:figure-compare-ruptures.png]]

The figure above shows that

- first column: changepoint is clearly cubic,
- second/third columns: ruptures is clearly quadratic in worst case,
  and best case seems to be somewhere between quadratic and
  log-linear.

** 12 July 2022

[[file:figure-timings-versions-data.R]] makes [[file:figure-timings-versions-data.csv]]

[[file:figure-timings-versions.R]] reads that and makes

[[file:figure-timings-versions.png]]

** 17 May 2022

[[file:figure-synthetic.R]] makes [[file:figure-synthetic.png]]

** 4 May 2022

[[file:figure-neuroblastoma-iterations-data.R]] makes [[file:figure-neuroblastoma-iterations-data.csv]] and [[file:figure-neuroblastoma-iterations-bounds.csv]]

[[file:figure-neuroblastoma-iterations.R]] makes

[[file:figure-neuroblastoma-iterations.png]]

** 21 Apr 2022

[[file:figure-mcgill-iterations-data.R]] makes [[file:figure-mcgill-iterations-data.csv]]

[[file:figure-mcgill-iterations.R]] reads that and makes

[[file:figure-mcgill-iterations.png]]

Figure above shows that in real data achieves asymptotic best case
complexity.

** 13 Apr 2022

[[file:figure-best-heuristics.R]] makes figures below

[[file:figure-best-heuristics.png]]

[[file:figure-best-heuristics-segs-constant.png]]

[[file:figure-optimal-trees.R]] makes small pictures for slides like this

[[file:figure-optimal-trees-71.png]]

and big picture below 

[[file:figure-optimal-trees.png]]

** 11 Apr 2022

[[file:figure-compare-distributions.R]] makes

[[file:figure-compare-distributions.png]]

Figure above shows small asymptotic slowdown for Laplace median
distribution.

[[file:figure-splits-loss.R]] makes new figure

[[file:figure-splits-loss-cum.png]]

which shows some empirical cumulative counts smaller than the best,
which is possible since the "best" case time complexity bound refers
to the total number of splits at the end, not at each iteration.

** 6 Apr 2022

[[file:figure-splits-loss.R]] makes

[[file:figure-splits-loss.png]]

Figure above shows that 0/1 seq can be used for worst case of
l1,mean_norm,poisson loss, but for meanvar_norm we need 0/1/10/11
seq. Also linear data achieves best case for normal losses, and is
very close to best case for l1 and poisson. TODO figure out a simple
synthetic data sequence which achieves the best case for l1 and
poisson.

[[file:figure-timings-laplace-data.R]] makes [[file:figure-timings-meanvar_norm-data.csv]]

[[file:figure-timings-laplace.R]] reads that file and makes

[[file:figure-timings-laplace.png]]

Figure above shows that ruptures looks asymptotically slower in best
case. 
   
[[file:figure-timings-meanvar_norm-data.R]] makes [[file:figure-timings-meanvar_norm-data.csv]]

[[file:figure-timings-meanvar_norm.R]] reads that file and makes

[[file:figure-timings-meanvar_norm.png]]

Figure above shows that
- blockcpd is about the same as binsegRcpp multiset.
- for worst case changepoint is faster up to very large model sizes,
  but asymptotically slower. 

[[file:figure-timings-poisson-data.R]] makes [[file:figure-timings-poisson-data.csv]]

[[file:figure-timings-poisson.R]] reads that file and makes

[[file:figure-timings-poisson.png]]

Figure above shows that
- blockcpd about the same as binsegRcpp multiset.
- others consistent with other losses.

TODO compare both versions of blockcpd. Also compare with
max.segs=n.data since that is what blockcpd does?

** 24 Mar 2022

[[file:figure-neuroblastoma.R]] makes the figure below, which shows a real
data set for which there are differences between binsegRcpp and
ruptures/changepoint.

[[file:figure-neuroblastoma.png]]

** 23 Mar 2022

[[file:ruptures_bug.py]] and [[file:changepoint.bug.R]] used to report issues,
https://github.com/deepcharles/ruptures/issues/242 and
https://github.com/rkillick/changepoint/issues/69

** 22 Mar 2022

[[file:figure-timings-data.R]] makes [[file:figure-timings-data.csv]]

[[file:figure-timings.R]] reads that and makes

[[file:figure-timings.png]]

Figure above was created using synthetic data which achieve the
best/worst case time complexity of the binary segmentation
algorithm. For each data set of a given size N in
{2^2=4,8,16,32,...,2^20=1,048,576}, we run binary segmentation up to a
max of N/2 segments (and not going to a larger N if the algo/case
resulted in a time greater than 100 seconds). The timings suggest that
changepoint R package uses a cubic algorithm (three nested for loops)
whereas binsegRcpp uses an algorithm which is log-linear in the best
case, and quadratic in the worst case. The ruptures python module
seems to be asymptotically faster than changepoint but slower than
binsegRcpp, maybe quadratic?

[[file:figure-timings-loss.png]]

Figure above shows that loss for binsegRcpp is always less than loss
for others, suggesting that there are bugs in the other
implementations.

** 20 Jan 2022

[[file:figure-select-segments-data.R]] computes simulations using a
variety of model selection criteria, saving results to
[[file:figure-select-segments-data.csv]]

[[file:figure-select-segments.R]] reads that result CSV file and makes 

[[file:figure-select-segments.png]]
